{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "from skimage import data\n",
    "from skimage import filters, io\n",
    "from skimage.filters import try_all_threshold\n",
    "from scipy import ndimage as ndi\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IBM Plex Sans Arabic:   0%|          | 0/1000 [00:00<?, ?it/s]c:\\Users\\Momo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\util\\dtype.py:576: UserWarning: Downcasting int32 to uint8 without scaling because max value 255 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n",
      "Processing IBM Plex Sans Arabic: 100%|██████████| 1000/1000 [13:32<00:00,  1.23it/s]\n",
      "Processing Lemonada: 100%|██████████| 1000/1000 [14:09<00:00,  1.18it/s]\n",
      "Processing Marhey: 100%|██████████| 1000/1000 [13:14<00:00,  1.26it/s]\n",
      "Processing Scheherazade New: 100%|██████████| 1000/1000 [13:15<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_data: (4000, 1181, 1181) shape of y_data: (0,)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "x_data = []\n",
    "y_data = []\n",
    "Fonts = [ 'IBM Plex Sans Arabic', 'Lemonada', 'Marhey','Scheherazade New'] \n",
    "\n",
    "for font in Fonts:\n",
    "    filenames = sorted(glob.glob(f'./fonts-dataset/{font}/*.jpeg'))\n",
    "    for filename in tqdm(filenames, desc=f\"Processing {font}\"):\n",
    "       img =ski.io.imread(filename)   \n",
    "       filtered_img = ski.filters.median(img,mode='nearest')\n",
    "       grayImg = ski.color.rgb2gray(filtered_img)\n",
    "       thresh= ski.filters.threshold_otsu(grayImg)#neb2a nshof meen\n",
    "       binary = grayImg < thresh\n",
    "       binaryImg = img = ski.util.img_as_ubyte(binary)\n",
    "       black_pixels = np.sum(binaryImg == 0)\n",
    "       total_pixels = binaryImg.size\n",
    "       percentage_black = (black_pixels / total_pixels) * 100\n",
    "       if percentage_black > 50:\n",
    "           inverted_binaryImg = np.where(binaryImg == 0, 255, 0)\n",
    "       else:\n",
    "           inverted_binaryImg = binaryImg\n",
    "       inverted_binaryImg = ski.util.img_as_ubyte(inverted_binaryImg)\n",
    "       edges = ski.feature.canny(img, sigma=4)\n",
    "       h, theta, d = ski.transform.hough_line(edges)\n",
    "       accum, angles, dists = ski.transform.hough_line_peaks(h, theta, d)\n",
    "       # Calculate the mode of the angles\n",
    "       if len(angles) > 0:\n",
    "           amgles_deg = np.rad2deg(angles)+90\n",
    "           angl_mode = stats.mode(amgles_deg)\n",
    "           rotation_angle = angl_mode.mode\n",
    "       \n",
    "           # Rotate the image\n",
    "           rotated_img = ski.transform.rotate(inverted_binaryImg, rotation_angle, cval=1,resize=False)\n",
    "           rotated_img = ski.util.img_as_ubyte(rotated_img)\n",
    "       else:\n",
    "           # Skip the rotation if no angles were found\n",
    "           rotated_img = inverted_binaryImg\n",
    "       x_data.append(rotated_img)\n",
    "       \n",
    "       \n",
    "x_data = np.asarray(x_data)\n",
    "y_data = np.asarray(y_data)\n",
    "\n",
    "\n",
    "print(\"shape of x_data:\", x_data.shape, \"shape of y_data:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IBM Plex Sans Arabic: 100%|██████████| 1000/1000 [00:00<?, ?it/s]\n",
      "Processing Lemonada: 100%|██████████| 1000/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Marhey: 100%|██████████| 1000/1000 [00:00<?, ?it/s]\n",
      "Processing Scheherazade New: 100%|██████████| 1000/1000 [00:00<00:00, 545636.01it/s]\n"
     ]
    }
   ],
   "source": [
    "font_to_num = {font: i for i, font in enumerate(Fonts)}\n",
    "\n",
    "y_data = []\n",
    "for font in Fonts:\n",
    "    filenames = sorted(glob.glob(f'./fonts-dataset/{font}/*.jpeg'))\n",
    "    for filename in tqdm(filenames, desc=f\"Processing {font}\"):\n",
    "        y_data.append(font_to_num[font])\n",
    "\n",
    "y_data = np.asarray(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Assuming preprocessed_images is a list of your preprocessed images\n",
    "preprocessed_images = x_data\n",
    "\n",
    "# Specify the directory you want to save the images in\n",
    "save_dir = \"preprocessed_images\"\n",
    "\n",
    "# Make sure the directory exists, if not, create it\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Loop through your preprocessed images\n",
    "for i, img in enumerate(preprocessed_images):\n",
    "    # Convert the image to uint8\n",
    "    img_uint8 = (img).astype(np.uint8)\n",
    "    \n",
    "    # Define the name of the file\n",
    "    filename = os.path.join(save_dir, f\"{y_data[i]}_{i}.jpg\")\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(filename, img_uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Specify the directory where your preprocessed images are saved\n",
    "img_dir = \"preprocessed_images\"\n",
    "\n",
    "# Get the list of image file names\n",
    "img_files = os.listdir(img_dir)\n",
    "\n",
    "# Initialize an empty list to store the images\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "# Loop through the image files\n",
    "for img_file in img_files:\n",
    "    # Construct the full path of the image file\n",
    "    img_path = os.path.join(img_dir, img_file)\n",
    "    \n",
    "    # Read the image using cv2.imread\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Use cv2.IMREAD_COLOR for color images\n",
    "    \n",
    "    # Append the image to x_data\n",
    "    x_data.append(img)\n",
    "    label=img_file.split('_')[0]\n",
    "    y_data.append(label)\n",
    "\n",
    "# Convert x_data to a numpy array\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data) \n",
    "\n",
    "\n",
    "# Now you can use x_data in your code\n",
    "p = np.random.permutation(len(x_data))\n",
    "x_data = x_data[p]\n",
    "y_data = y_data[p]\n",
    "x_images = x_data\n",
    "del img_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 41/41 [05:07<00:00,  7.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from cv2 import resize\n",
    "import skimage as ski\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "\n",
    "hog_features = []\n",
    "\n",
    "batch_size = 100  # Adjust this based on your available memory\n",
    "num_batches = len(x_data) // batch_size\n",
    "\n",
    "for i in tqdm(range(num_batches + 1), desc=\"Processing batches\"):  # +1 to include the last batch\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, len(x_data))  # Ensure the end index doesn't exceed the number of images\n",
    "    batch = x_data[start:end]\n",
    "    \n",
    "    for image in batch:\n",
    "        # Resize the image\n",
    "        image = resize(image, (1000, 1000))\n",
    "        \n",
    "        # Apply HOG feature extraction\n",
    "        hog_feat = ski.feature.hog(image, orientations=15, pixels_per_cell=(70,55), cells_per_block=(14, 11), block_norm='L2')\n",
    "        hog_features.append(hog_feat)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = np.hstack([hog_features])\n",
    "normalized_features = scaler.fit_transform(hog_features)\n",
    "\n",
    "#save the features\n",
    "# np.save('hog_features.npy', hog_features)\n",
    "# np.save('sift_features.npy', sift_features_padded)\n",
    "np.save('normalized_features.npy', normalized_features)\n",
    "\n",
    "del x_data , hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (2800, 18480) shape of y_train: (2800,)\n",
      "shape of x_val: (1200, 18480) shape of y_val: (1200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train = np.load('normalized_features.npy')\n",
    "# First split the data into a training set and a temporary set using a 70-30 split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_data, test_size=0.3, random_state=22, stratify=y_data)\n",
    "x_images_tarin ,x_images_val, y_images_train, y_images_val = train_test_split(x_images, y_data, test_size=0.3, random_state=22, stratify=y_data)\n",
    "# Then split the temporary set into a validation set and a test set using a 50-50 split\n",
    "\n",
    "\n",
    "print(\"shape of x_train:\", x_train.shape, \"shape of y_train:\", y_train.shape)\n",
    "print(\"shape of x_val:\", x_val.shape, \"shape of y_val:\", y_val.shape)\n",
    "\n",
    "# del normalized_features, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components=0.9999)  # Adjust this to the desired number of components or variance to retain\n",
    "# Fit the PCA model and transform the data\n",
    "features_pca = pca.fit(x_train)\n",
    "features_pca = pca.transform(x_train)\n",
    "\n",
    "del x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels to integers\n",
    "np.save('features_pca.npy', features_pca)\n",
    "le = LabelEncoder()\n",
    "y_data_encoded = le.fit_transform(y_train[:len(features_pca)])  # Ensure the number of labels matches the number of features\n",
    "\n",
    "# Create an SVM classifier with gamma='scale' and C=1\n",
    "clf = svm.SVC(gamma='scale', C=6.4)\n",
    "\n",
    "# Fit the classifier on the data\n",
    "clf.fit(features_pca, y_data_encoded)\n",
    "\n",
    "del features_pca, y_data_encoded\n",
    "\n",
    "# Now the SVM model is trained and ready to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 95.50%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score# Apply PCA\n",
    "features_pca = pca.transform(x_val)\n",
    "\n",
    "# Encode the validation labels\n",
    "y_val_encoded = le.transform(y_val)\n",
    "\n",
    "# Use the trained SVM model to make predictions on the validation set\n",
    "\n",
    "y_pred = clf.predict(features_pca)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_val_encoded, y_pred)\n",
    "\n",
    "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_val: ['0' '3' '0' ... '2' '2' '0']\n",
      "y_pred: [0 3 0 ... 2 2 0]\n",
      "y_val_enc: [0 3 0 ... 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "features_pca.shape\n",
    "print(f'y_val: {y_val}')\n",
    "print(f'y_pred: {y_pred}')\n",
    "print(f'y_val_enc: {y_val_encoded}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincorrect_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# If the directory already exists, delete its contents\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincorrect_predictions/*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m     11\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "from skimage.io import imsave\n",
    "import os\n",
    "\n",
    "# Create a directory to save the images\n",
    "if not os.path.exists('incorrect_predictions'):\n",
    "    os.makedirs('incorrect_predictions')\n",
    "else:\n",
    "    # If the directory already exists, delete its contents\n",
    "    files = glob.glob('incorrect_predictions/*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "# Iterate over the predictions and actual labels\n",
    "for i in range(len(y_pred)):\n",
    "    # If the prediction is incorrect\n",
    "    if y_pred[i] != y_val_encoded[i]:\n",
    "        # Save the corresponding image\n",
    "        imsave(f'incorrect_predictions/img_{i}.png', x_images_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
